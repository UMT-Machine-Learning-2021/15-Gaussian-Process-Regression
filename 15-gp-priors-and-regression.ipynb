{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aggregate-archive",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "## Synthetic Examples\n",
    "\n",
    "In this lab, we'll explore GP priors, as well as regressing on some synthetic data.  For this lab, we'll only use numpy and matplotlib, underscoring the apparent simplicity of this approach to modelling.  \n",
    "\n",
    "### GP Priors\n",
    "\n",
    "As a conceptual example, let's again imagine that we're predicting the (log-)concentration of gold in the surface layer of a landscape (although you could envision this as any other spatial interpolation task that you might find more compelling).  To begin, let's define an index set $X^*$, or locations at which we'd like to evaluate a model.  For simplicity's sake, let's say that our domain is the interval $[-1,1]$, and that we'd like to evaluate this model at $m^*=100$ evenly spaced points.  Then we can define\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinated-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "m_star = 100\n",
    "X_star = np.linspace(-1,1,m_star).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-crossing",
   "metadata": {},
   "source": [
    "Next, create a function that produces the covariance matrix\n",
    "$$\n",
    "K(X,X') = \\begin{bmatrix} k(x_1,x_1) & k(x_1,x_2) & \\cdots & k(x_1,x_m) \\\\\n",
    "                          k(x_2,x_1) & k(x_2,x_2) & \\cdots & k(x_2,x_m) \\\\\n",
    "                          \\vdots     & \\vdots     & \\ddots & \\vdots     \\\\\n",
    "                          k(x_m,x_1) & k(x_m,x_2) & \\cdots & k(x_m,x_m) \\end{bmatrix}\n",
    "$$\n",
    "for covariance function\n",
    "$$\n",
    "k(x,x') = \\sigma^2 \\mathrm{exp}\\left( - \\frac{1}{2} \\frac{|x-x'|^2}{\\ell^2} \\right).\n",
    "$$\n",
    "This functions should not only accept vectors $X$ and $X'$, but should also accept hyperparameters $\\sigma^2$ and $\\ell$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix # You might find this useful for a vectorized implementation\n",
    "\n",
    "def k(x,xp,l=0.3,sigma2=10.0):\n",
    "    # Do stuff here\n",
    "    return None # CHANGE ME\n",
    "\n",
    "K = k(X_star,X_star)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-kentucky",
   "metadata": {},
   "source": [
    "Evaluate your covariance function on $X^*$, which should create a $m^* \\times m^*$ symmetric matrix.  The diagonal of this covariance matrix contains the so-called 'marginal' variance at each point: the *a prior* uncertainty in the function value.  Given this covariance function, you can use the following function to produce a nice plot of the resulting GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_plot(X,m,K):\n",
    "    plt.plot(X_star.squeeze(),m.squeeze(),'r--',lw=3.0,label='mean')\n",
    "    plt.fill_between(X_star.squeeze(),-3*np.sqrt(np.diagonal(K)),3*np.sqrt(np.diagonal(K)),facecolor='k',alpha=0.2)\n",
    "    plt.fill_between(X_star.squeeze(),-2*np.sqrt(np.diagonal(K)),2*np.sqrt(np.diagonal(K)),facecolor='k',alpha=0.2)\n",
    "    plt.fill_between(X_star.squeeze(),-1*np.sqrt(np.diagonal(K)),1*np.sqrt(np.diagonal(K)),facecolor='k',alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Gold Concentration')\n",
    "    \n",
    "gp_plot(X_star,np.zeros_like(X_star),K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-hawaiian",
   "metadata": {},
   "source": [
    "This isn't very interesting on its own: it's much more illuminating to draw samples from the GP.  Write a function called 'sample' that takes a mean vector and covariance matrix, and draws a random sample from the GP prior.  To do this, draw random Gaussian noise $Z$ of size $n_samples \\times m_star$.  Then compute the eigenvalues $\\lambda$ and eigenvectors $V$ of the covariance matrix.  Samples from the GP can then be computed as \n",
    "\n",
    "$$\n",
    "Y_{sample} = Z \\mathrm{diag}(\\sqrt{\\lambda}) V^T\n",
    "$$\n",
    "NOTE 1: you might get complex values out of numpy's eig function.  you can take just the real part using variable.real.  NOTE 2: if $\\lambda$ has any negative entries, you'll get nans: to deal with this problem, set any negative eigenvalues to zero.  \n",
    "\n",
    "\n",
    "Use this function to draw a few samples and plot them on top of the plot above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(m,K,n_samples=10):\n",
    "    # Do some stuff here\n",
    "    return None # You'll want to change this\n",
    "\n",
    "sample(np.zeros_like(X_star),K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-teaching",
   "metadata": {},
   "source": [
    "Explore this procedure for a few different values of $\\ell$ and $\\sigma^2$.  **Write a few lines describing what changing these hyperparameters does**.  Another interesting covariance function is the exponential covariance\n",
    "$$\n",
    "k_{exp}(x,x') = \\sigma^2 \\exp \\left( -\\frac{|x-x'|}{\\ell} \\right).\n",
    "$$\n",
    "Repeat the above experiments using this covariance instead.  **How does this change alter the character of the sampled function?**\n",
    "\n",
    "### GP Regression\n",
    "We would now like to use these functions to regress on some data.  Imagine that we've sent out miners to assay the soil and they return with the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "delayed-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-0.5,0.0,0.5,0.6,0.9]]).T\n",
    "Y = np.array([[0.2,0.1,0.8,0.8,0.3]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-wiring",
   "metadata": {},
   "source": [
    "Compute the posterior mean and covariance of \n",
    "$$P(Y^*|X^*,X,Y).$$\n",
    "The formulae for these are\n",
    "$$\n",
    "\\mu_{post} = K(X^*,X) [K(X,X) + \\sigma^2_{obs} I]^{-1} Y\n",
    "$$\n",
    "$$\n",
    "K_{post} = K(X^*,X^*) - K(X^*,X) [K(X,X) + \\sigma^2_{obs} I]^{-1} K(X,X^*)\n",
    "$$\n",
    "Begin with $\\sigma^2 = 0.01$.\n",
    "\n",
    "Using the plotting and sampling functions from above, plot the mean and marginal variance of these predictions as a function of space.  Include the observations on these plots as well.  Explore different values of $\\sigma^2$, $\\ell$ and discuss what you find to be values that fit the data well.  Try both squared-exponential and exponential covariance functions.  **Once you've decided on hyperparameters, write a few sentences describing the uncertainty and its relationship to the distance from the nearest datapoint.**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-cliff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-barrier",
   "metadata": {},
   "source": [
    "### Bonus Round (Optional)\n",
    "Recreate a situation similar to the above, but in 2D (rather than 1D) spatial coordinates.  How does the process change (if at all)?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
